#!/usr/bin/env python3
"""
Neural-network baseline for the dog behaviour dataset.

Two data sources are supported:
1. processed_dog_features.pkl  (generated by process_dog_data.py, matches the Matlab pipeline)
2. dog_features.csv            (older generic feature file)

Example usage:
    python nn_dog_classifier.py --data processed_dog_features.pkl --sensor Back
    python nn_dog_classifier.py --data dog_features.csv

The script performs Leave-One-Dog-Out (LODO) evaluation to stay aligned with
the original paper. Each fold trains a small feed-forward network from
scratch on the remaining dogs and reports per-fold / overall metrics.
"""

import argparse
import pickle
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import torch
from sklearn.metrics import accuracy_score, f1_score
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.preprocessing import LabelEncoder, StandardScaler
from torch import nn
from torch.utils.data import DataLoader, Dataset


# ---------------------------------------------------------------------------
# Data loading helpers
# ---------------------------------------------------------------------------
def load_from_processed(path: Path, sensor: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    with open(path, "rb") as fh:
        payload = pickle.load(fh)
    sensor = sensor.capitalize()
    if sensor not in payload["sensors"]:
        raise ValueError(f"Sensor '{sensor}' not found in {path}. Available: {list(payload['sensors'].keys())}")
    sensor_frame = payload["sensors"][sensor]
    if isinstance(sensor_frame, pd.DataFrame):
        X = sensor_frame.to_numpy(dtype=np.float32)
    else:
        X = np.asarray(sensor_frame, dtype=np.float32)
    y = np.asarray(payload["labels"])
    groups = np.asarray(payload["dog_ids"])
    return X, y, groups


def load_from_csv(path: Path) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    df = pd.read_csv(path)
    if "dog_id" not in df or "label" not in df:
        raise ValueError("CSV file must include 'dog_id' and 'label' columns.")
    feature_cols = [c for c in df.columns if c not in {"dog_id", "test_num", "window_start", "window_end", "label"}]
    if not feature_cols:
        raise ValueError("No feature columns found in CSV.")
    X = df[feature_cols].to_numpy(dtype=np.float32)
    y = df["label"].to_numpy()
    groups = df["dog_id"].to_numpy()
    return X, y, groups


def load_dataset(path: str, sensor: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    path_obj = Path(path)
    if not path_obj.exists():
        raise FileNotFoundError(f"{path} not found.")
    if path_obj.suffix == ".pkl":
        return load_from_processed(path_obj, sensor)
    return load_from_csv(path_obj)


# ---------------------------------------------------------------------------
# Torch dataset/model utilities
# ---------------------------------------------------------------------------
class DogDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.from_numpy(X).float()
        self.y = torch.from_numpy(y).long()

    def __len__(self) -> int:
        return len(self.X)

    def __getitem__(self, idx: int):
        return self.X[idx], self.y[idx]


class DogNet(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, num_classes: int, dropout: float = 0.2):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, num_classes),
        )

    def forward(self, x):
        return self.net(x)


def train_one_epoch(model, loader, criterion, optimizer, device) -> float:
    model.train()
    total_loss = 0.0
    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        logits = model(xb)
        loss = criterion(logits, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * len(xb)
    return total_loss / len(loader.dataset)


def evaluate(model, loader, device) -> Tuple[np.ndarray, np.ndarray]:
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for xb, yb in loader:
            xb = xb.to(device)
            logits = model(xb)
            pred = logits.argmax(dim=1).cpu().numpy()
            preds.append(pred)
            trues.append(yb.numpy())
    return np.concatenate(trues), np.concatenate(preds)


# ---------------------------------------------------------------------------
# Main training loop (LODO evaluation)
# ---------------------------------------------------------------------------
def run_lodo_training(X: np.ndarray, y: np.ndarray, groups: np.ndarray, args) -> Dict[str, float]:
    device = torch.device("cuda" if torch.cuda.is_available() and not args.cpu else "cpu")
    label_encoder = LabelEncoder()
    y_enc = label_encoder.fit_transform(y)
    logo = LeaveOneGroupOut()

    all_true, all_pred = [], []
    for fold, (train_idx, test_idx) in enumerate(logo.split(X, y_enc, groups), start=1):
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y_enc[train_idx], y_enc[test_idx]

        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        train_dataset = DogDataset(X_train, y_train)
        test_dataset = DogDataset(X_test, y_test)
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size)

        model = DogNet(
            input_dim=X.shape[1],
            hidden_dim=args.hidden_dim,
            num_classes=len(label_encoder.classes_),
            dropout=args.dropout,
        ).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        criterion = nn.CrossEntropyLoss()

        for epoch in range(1, args.epochs + 1):
            loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
            if args.verbose and (epoch % max(1, args.log_interval) == 0):
                print(f"[Fold {fold:02d}] Epoch {epoch:03d} - loss={loss:.4f}")

        true_fold, pred_fold = evaluate(model, test_loader, device)
        acc = accuracy_score(true_fold, pred_fold)
        f1 = f1_score(true_fold, pred_fold, average="macro")
        print(f"[Fold {fold:02d}] Dog {np.unique(groups[test_idx])[0]} - Acc={acc:.3f}, F1_macro={f1:.3f}")
        all_true.append(true_fold)
        all_pred.append(pred_fold)

    all_true = np.concatenate(all_true)
    all_pred = np.concatenate(all_pred)
    overall_acc = accuracy_score(all_true, all_pred)
    overall_f1 = f1_score(all_true, all_pred, average="macro")
    print(f"\nOverall: Acc={overall_acc:.3f}, F1_macro={overall_f1:.3f}")
    return {"accuracy": overall_acc, "f1_macro": overall_f1}


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="Neural network baseline for dog behaviour recognition.")
    parser.add_argument("--data", default="processed_dog_features.pkl", help="Path to processed .pkl or raw CSV file.")
    parser.add_argument("--sensor", default="Back", help="Sensor name when using processed_dog_features.pkl (Back/Neck).")
    parser.add_argument("--epochs", type=int, default=50, help="Training epochs per fold.")
    parser.add_argument("--hidden-dim", type=int, default=128, dest="hidden_dim", help="Hidden layer size.")
    parser.add_argument("--dropout", type=float, default=0.2, help="Dropout rate.")
    parser.add_argument("--batch-size", type=int, default=128, dest="batch_size", help="Mini-batch size.")
    parser.add_argument("--lr", type=float, default=1e-3, help="Learning rate.")
    parser.add_argument("--weight-decay", type=float, default=1e-4, dest="weight_decay", help="Adam weight decay.")
    parser.add_argument("--cpu", action="store_true", help="Force CPU even if CUDA is available.")
    parser.add_argument("--log-interval", type=int, default=10, dest="log_interval", help="Epoch interval for logging.")
    parser.add_argument("--verbose", action="store_true", help="Print training loss during epochs.")
    return parser.parse_args()


def main():
    args = parse_args()
    X, y, groups = load_dataset(args.data, args.sensor)
    print(f"Loaded {X.shape[0]} samples with {X.shape[1]} features from {args.data}")
    run_lodo_training(X, y, groups, args)


if __name__ == "__main__":
    main()
